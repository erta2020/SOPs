{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Soil Texture Processing File\"\nauthor: \"Marc Los Huertos\"\ndate: \"7/26/2016\"\noutput:\n  html_document: default\n  word_document: default\n---\n\n# Prepare and and Start Functions for Processing\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n\nlibrary(mosaic)\nlibrary(dplyr)\n\n# source(\"Soil_functions/ChooseRead_fun.R\")\nsource(\"Soil_functions/PassNo10_fun.R\")\nsource(\"Soil_functions/RemoveMissingKEYs_fun.R\")\nsource(\"Soil_functions/Impute_fun.R\")\nsource(\"Soil_functions/Interpolation_fun.R\")\nsource(\"Soil_functions/SoilEquiv_fun.R\")\nsource(\"Soil_functions/TimeConversion_fun.R\")\nsource(\"Soil_functions/Kfun.R\")\nsource(\"Soil_functions/Hygro_fun.R\")\nsource(\"Soil_functions/EffectiveSoilWeight_fun.R\")\n```\n\n## Uploading and Downloading the Data\n\n[Database Access](http://thinkertools.org/grainsize/) \n\n\"18Pomona87\"\n\nshiny app\n\n(https://marclos.shinyapps.io/ParticleSizeAnalysis/)\n\n## Copy the File into correct\n\n## Read reading.csv Data into R\n\nIn spite of myself, there is no clear way to automate the file selection process. So, here are the steps to get our csv files into R.\n\nHere's what I do:\n\nuse file.choose() to get the file and path for both the sample.csv and reading.csv. Then create an object, \"file\" with the path/file, see my example below:\n\nfile.choose()\n\nThen we can import the data, using this file!\n\n```{r readcsv}\nsample.source = \"/home/CAMPUS/mwl04747/github/SOPs/32_Soil_Texture_Analysis/Data/sample.csv\"\nreading.source = \"/home/CAMPUS/mwl04747/github/SOPs/32_Soil_Texture_Analysis/Data/reading.csv\"\n\nsample.import <- read.csv(sample.source)\nreading.import <- read.csv(reading.source)\n# Clean up Environment\nrm(reading.source, sample.source)\n\n```\n\n## Check Data Importing\n\n### What are the two data sets?\n\n### Removing Missing Data\n\nThe function RemoveMissingKEYs look at the two files and removes samples that have missing key measurments that will make the program crash.\n\n```{r, echo=F}\nsample <- RemoveMissingKEYs(sample.import, reading.import)\n```\n\n# Preprocess Sample File\n\n## Calculate Percent Soil Passing Seive No. 10\n```{r preprocess}\n\nsample$passNo10Percent <- PassNo10(sample$pass10, sample$totalSoil)\n```\n\n## Calculate Percent Hygroscopic Water\n\n```{r hygroscopic}\nsample$hygroPercent2 = hygro(sample$tinAir, sample$tinOven, sample$tinTare); #head(sample)\nsample$WS_e = WS_e(sample$tinAir, sample$tinOven, sample$tinTare, sample$actualSoil);\n```\n\n## Assign Soil Particle Density if Missing\n```{r soildensity}\nsample$GSp = 2.65\n```\n\n## Select Soil Parameters for Processing Reading File\n```{r soilparams}\nSampleParams <- sample %>% select(sampleKEY, WS_e, passNo10Percent, hygroPercent2, GSp); \n#head(SampleParams)\n  \n```\n\n# Process Hydrometer Readings\n\n## Preprocess Reading File\n\n\n### Remove badKEYs from reading\n```{r preprocess_reading}\njoined <- inner_join(reading.import, SampleParams, by=\"sampleKEY\")\n```\n\n### Convert Et to Minutes\n```{r elapsedtime}\njoined$Et <- round(joined$timeElapsed * ElapsedTimeConversion(joined$timeUnit), 2)\n```\n\n### Subset joined file\n```{r subsetjoin}\njoined2 <- joined %>% select(sampleKEY, passNo10Percent, GSp, Et, WS_e, actual, blank, temp) \n```\n\n## Join SampleParams and Reading SECTION IS MOOT\n\nFirst, I created two files that have only a few columns each so we can test the join function.\n\n```{r joinkeyID}\n#reading.select <- select(reading, sampleKEY, Et, actual, blank, temp); #head(reading.select)\n\n#joined2 <- left_join(SampleParams, reading.select, by = \"sampleKEY\")\n```\n\n## Loop sampleKEY to Impute Missing Temp and Blanks\n\n## Using Non-Missing to Impute Missing Temp and RBlank\n\nI still believe there is a more elegant to do this, but this is better than my initial idea!\n\n### Ways to Improve the Function:\n\nFirst, we could make the function that address both missing values simultaneously. For example, impute(sampleID, parameter)...  I tried this, but ran into the problem that we use different columns that are identified in differeing ways. Perhaps, next version!\n\n```{r imputefunction}\njoined <- impute(joined2)\n```\n\n## Calculate the Corrected Hydrometer Reading\n\nNot sure what we are doing here... Isaac, can you comment?\n\nCorrected Rc is based on the formula...\n\n\n\n```{r CorrectedReading}\njoined$Rc <- joined$actual - (joined$blank2 - 1)\n```\n\n## Calculating K\n\n```{r calculatingK}\n\njoined$K <- Kfun(round(joined$temp2,0)); head(joined)\n```\n\n\n## Calculate Effective Depth\n\nEffective Depth...\n\n```{r effectivedepth}\n# For High Density Hydrometers\n# x = c(1.00, 1.07)\n# y = c(15.5, 0.6)\n\n# For H151\nx = c(1.00, 1.031)\ny = c(10.5, 2.3)\nL2coef = coef(lm(y~x))\nL1 = joined$Rc * L2coef[2] + L2coef[1]\nL2 = 14.0\nVb = 67\nA = 27.8\n\njoined$EffectiveDepth = L1 + 0.5*(L2-(Vb/A))\n\nhead(joined)\n\nrm(x, y, L2coef, L1, L2, Vb, A, b, m, x1)\n\n```\n## Calculate D\nAccording to ASTM D422-63 the equation to calculate D (diameter in mm) can be  simplified as D = K*sqrt(L/T) where: L=effective depth (Leff), T = measurment time in min and K is the constant given in the tables that varies with temperature (already calculated).\n\n```{r De}\njoined$De<-joined$K * sqrt(joined$EffectiveDepth/joined$Et)\nhead(joined)\n```\n\n## Calculate PF\n\nP=[(100,000/w)*G/(G-G1)]*(R-G1) (pretty sure G1=1, pretty sure R = Rc (ask marc about this one)) and G=2.65 g/cm^3 (ask marc about this one too)\n\nThe bracketed portion will be the same for all variables thus we can calculate it separately and call it p1 (1st part of equation for )\n\n```{r perfecentfiner}\njoined$W = joined$WS_e/joined$passNo10Percent * 100\njoined$PF= ((100000/joined$W)*(joined$GSp/(joined$GSp-1)))*(joined$Rc - 1)\n```\n\n## Creating Figures\n\nYou can also embed plots, for example:\n\n```{r figure, echo=T}\nKEY <- unique(joined$sampleKEY)\nKEY <- KEY[-c(1:25)]\n           \nfor(i in 1:length(KEY)){\n#with(joined[joined$sampleKEY==KEY[i],], plot(x=log(De), y=PF, xlab=\"Log of Diameter (mm)\", ylab=\"Percent Finer\", las=1))\n}\n\n```\n\n## Estimating the Texture Size Classes\n\n```{r interpolation}\n\nKEYs <- unique(joined$sampleKEY); KEYs\n\ntextureclasses <- matrix(ncol=4, nrow=length(KEYs))\nclay <- 0.002; silt <- 0.05; sand <- 2.0\n\nfor(i in 1:length(KEYs)){\ntmp3 = joined[joined$sampleKEY==KEYs[i],]; head(tmp3)\ntmp3 = select(joined, sampleKEY, De, PF) %>% filter(joined$sampleKEY==KEYs[i]); tmp3 \n\nPerclay = round(Interpolation(tmp3, clay),1)\nPersilt = round(Interpolation(tmp3, silt), 1)\nPersand = round(100-(Persilt + Perclay), 1)\ntextureclasses[i,] <- c(KEYs[i], Persand, Persilt, Perclay)\n}\n\nrm(KEYs, KEY, Perclay, Persand, Persilt, m, b, i)\ntextureclasses <- data.frame(textureclasses)\nnames(textureclasses) <- c(\"sampleKEY\", \"Sand\", \"Silt\", \"Clay\")\n\nclass(textureclasses)\n\nresults <- sample %>% select(sampleKEY, sampleID, sampleDate, researcher) %>% left_join(textureclasses, by=\"sampleKEY\")\n\nwrite.csv(results, file = paste(\"/home/CAMPUS/mwl04747/github/SOPs/32_Soil_Texture_Analysis/Data/results_\",Sys.Date(),\".csv\", sep=\"\"))\n\n#rm(sand,silt, clay, results, x1, y, L2coef, x)\n\n```\n\n",
    "created" : 1471465566290.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "125151823",
    "id" : "311F43EF",
    "lastKnownWriteTime" : 1471471469,
    "last_content_update" : 1471471469956,
    "path" : "~/github/SOPs/32_Soil_Texture_Analysis/Data_Processing_Guide_160716.Rmd",
    "project_path" : "32_Soil_Texture_Analysis/Data_Processing_Guide_160716.Rmd",
    "properties" : {
        "chunk_rendered_width" : "650",
        "last_setup_crc32" : "e9956de8725e52e5"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}